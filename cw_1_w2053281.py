# -*- coding: utf-8 -*-
"""CW_1_w2053281.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z26Bcrh6-tRCkMbcvPpQMIFNWinrzGBE

Import required libraries

This section imports all the necessary Python libraries for data analysis and visualization:

- pandas & numpy: For data manipulation and numerical computations
- matplotlib & seaborn: For creating visualizations and charts
- datetime & pathlib: For handling dates and file management
- requests: For downloading data from web sources
- warnings: Configured to suppress unnecessary warning messages for cleaner output
"""

import pandas as pd              # For working with data tables
import numpy as np               # For mathematical operations
import matplotlib.pyplot as plt  # For creating graphs and charts
import seaborn as sns           # For making prettier visualizations
from datetime import datetime    # For working with dates and times
from pathlib import Path        # For creating folders and managing file paths
import requests                 # For downloading files from the internet
import warnings                 # For hiding warning messages
warnings.filterwarnings('ignore')  # Turn off warnings to keep output clean

"""Displays a success message confirming that all required libraries have been imported and the data pipeline is ready to run."""

print("="*80)
print("UK ROAD SAFETY DATA PIPELINE - SETUP")
print("="*80)
print("✓ Libraries imported successfully\n")

"""Sets up default visualization preferences whitegrid style for clean backgrounds and standard figure size of 12x6 inches for all charts."""

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("-"*80)
print("Creating folder structure...")
print("-"*80)

"""Defines the directory structure for organizing raw data files (collisions, vehicles, casualties) and processed outputs. This ensures data is systematically organized throughout the pipeline."""

folders = [
      'data/raw/collisions',   # For collision data
     'data/raw/vehicles',     # For vehicle data
      'data/raw/casualties',   # For casualty data
     'data/processed'         # For cleaned data (used later)
 ]

"""Executes folder creation with error handling: creates missing directories, skips existing ones, and displays status updates for each path."""

for folder in folders:   # Create each folder (parents=True creates parent folders if needed)
     Path(folder).mkdir(parents=True, exist_ok=True)
     print(f"✓ Created: {folder}")
     print("\n✓ Folder structure ready!")

print("\n" + "-"*80)
print("Setting up data source URLs...")
print("-"*80)

"""Maps dataset names to their corresponding download URLs from the UK Department for Transport's open data portal. Each URL provides CSV files with the most recent 5 years of road safety statistics."""

# These URLs point to the official UK Department for Transport data repository
URLS = {
  'collisions': 'https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-collision-last-5-years.csv',
  'vehicles': 'https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-vehicle-last-5-years.csv',
  'casualties': 'https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-casualty-last-5-years.csv'
}

print("✓ URLs set")

"""Maps dataset names to their destination file paths in the local folder structure. This dictionary works alongside the URLs to manage where each downloaded file is stored."""

SAVE_PATHS = {
      'collisions': 'data/raw/collisions/collisions.csv',
      'vehicles': 'data/raw/vehicles/vehicles.csv',
      'casualties': 'data/raw/casualties/casualties.csv'
}

                  # Dictionary mapping dataset names to where we'll save them

"""Download Function
This function handles data extraction for Task 1.

Input: URL, save path, dataset name
Output: Downloads file and returns success status (True/False)
Error Handling: Catches exceptions and reports errors without crashing
"""

def download_file(url, save_path, dataset_name):
      """Download file from URL and save it"""
      print(f"Downloading {dataset_name} data...")
      try:
         response = requests.get(url)
         with open(save_path, 'wb') as file:
              file.write(response.content)
         print(f"✓ {dataset_name} downloaded successfully!")
         return True
      except Exception as e:
        print(f"✗ Error: {e}")
        return False

"""Executes the download function for each of the three datasets (collisions, vehicles, casualties) using the predefined URLs and save paths. Stores success/failure status for each download."""

# Download collision data
print("-"*80)
collision_success = download_file(
    URLS['collisions'],
        SAVE_PATHS['collisions'],
            'COLLISION'
            )

# Download vehicle data
print("\n" + "-"*80)
vehicle_success = download_file(
    URLS['vehicles'],
        SAVE_PATHS['vehicles'],
            'VEHICLE'
            )

# Download casualty data
print("\n" + "-"*80)
casualty_success = download_file(
    URLS['casualties'],
        SAVE_PATHS['casualties'],
            'CASUALTY'
            )

# Check if all downloads succeeded
print("\n" + "="*80)
print("DOWNLOAD SUMMARY")
print("="*80)

if collision_success and vehicle_success and casualty_success:
      print("\n SUCCESS! All 3 files downloaded!")

"""Loads the collision CSV file into a pandas DataFrame and displays dataset dimensions (rows and columns). Includes try-except error handling for file loading issues."""

print("\n Loading COLLISION data...")
try:
       collisions_df = pd.read_csv(SAVE_PATHS['collisions'], low_memory=False)
       print(f"    Loaded successfully")
       print(f"   Rows: {len(collisions_df):,}")
       print(f"   Columns: {len(collisions_df.columns)}")
except Exception as e:
      print(f"   ✗ Error: {e}")

print("\n Loading VEHICLE data...")
try:
        vehicles_df = pd.read_csv(SAVE_PATHS['vehicles'], low_memory=False)
        print(f"   ✓ Loaded successfully")
        print(f"   Rows: {len(vehicles_df):,}")
        print(f"   Columns: {len(vehicles_df.columns)}")
except Exception as e:
        print(f"   ✗ Error: {e}")

print("\n Loading CASUALTY data...")
try:
      casualties_df = pd.read_csv(SAVE_PATHS['casualties'], low_memory=False)
      print(f"   ✓ Loaded successfully")
      print(f"   Rows: {len(casualties_df):,}")
      print(f"   Columns: {len(casualties_df.columns)}")
except Exception as e:
      print(f"   ✗ Error: {e}")

"""Creates a function that calculates and displays dataset size metrics (rows, columns, total data points). Returns results as a dictionary for further analysis or storage."""

def analyze_data_dimensions(df, name):
    """
    Prints the total number of records (rows), attributes (columns),
    and total data points for a given dataset.
    """
    rows, cols = df.shape
    datapoints = rows * cols
    print(f"\n{name.upper()} Dataset:")
    print(f"   Total Records (Rows):    {rows:,}")
    print(f"   Total Attributes (Cols): {cols}")
    print(f"   Total Data Points:       {datapoints:,}")
    return {
        "Dataset": name,
        "Rows": rows,
        "Columns": cols,
        "Data Points": datapoints
    }

"""Runs dimension analysis on all three datasets, combines results into a summary DataFrame, and displays a comparative table of their sizes (rows, columns, data points). Or the shortest version:"""

collision_summary = analyze_data_dimensions(collisions_df, "Collision")
vehicle_summary   = analyze_data_dimensions(vehicles_df, "Vehicle")
casualty_summary  = analyze_data_dimensions(casualties_df, "Casualty")

dimension_summary_df = pd.DataFrame([collision_summary, vehicle_summary, casualty_summary])
display(dimension_summary_df)

"""Consolidated view of all dataset dimensions in one place. Shows:
- Individual dataset sizes (rows, columns, data points)
- Grand totals across all datasets
- Easy comparison between datasets

Creates a function to display column names and data types for any DataFrame. Executes it for all three datasets to show their complete field structures with formatted output.
"""

#  Function to print only column names and their data types
def show_column_dtypes(df, name):

    print("\n" + "="*80)
    print(f"{name.upper()} DATASET - FIELD NAMES & DATA TYPES")
    print("="*80)
    for col, dtype in df.dtypes.items():
        print(f"{col:<50} {dtype}")


#  Call for each dataset
show_column_dtypes(collisions_df, "Collision")
show_column_dtypes(vehicles_df, "Vehicle")
show_column_dtypes(casualties_df, "Casualty")

"""2-C Number & percentage of missing values for each attribute

Runs the analysis on all three datasets (collisions, vehicles, casualties) and stores results for further investigation or reporting.
"""

#  Function to calculate number and percentage of missing values per column
def missing_values_summary(df, name):

    print("\n" + "="*80)
    print(f"{name.upper()} DATASET - MISSING VALUES SUMMARY")
    print("="*80)

    total_missing = df.isnull().sum()
    percent_missing = (total_missing / len(df)) * 100

    summary_df = pd.DataFrame({
        'Field Name': df.columns,
        'Missing Count': total_missing.values,
        'Missing %': percent_missing.round(2).values
    })

    # Filter only columns that have at least one missing value
    summary_df = summary_df[summary_df['Missing Count'] > 0]

    if summary_df.empty:
        print(" No missing values found in this dataset.")
    else:
        print(summary_df.to_string(index=False))

    return summary_df


# Run for all datasets
collision_missing = missing_values_summary(collisions_df, "Collision")
vehicle_missing   = missing_values_summary(vehicles_df, "Vehicle")
casualty_missing  = missing_values_summary(casualties_df, "Casualty")

"""2-D - d. Descriptive statistics for numeric and non-numeric attributes

Function generating descriptive statistics for numeric (distribution metrics) and categorical (frequency counts) columns. Applied to all datasets with transposed output for readability.
"""

#Function to show descriptive statistics for numeric and non-numeric attributes
def descriptive_statistics(df, name):

    print("\n" + "="*80)
    print(f"{name.upper()} DATASET - DESCRIPTIVE STATISTICS")
    print("="*80)

    #  Numeric Attributes
    numeric_summary = df.describe().T  # Transposed for readability
    print("\n NUMERIC ATTRIBUTES SUMMARY:")
    print(numeric_summary.to_string())

    #  Non-Numeric Attributes
    categorical_df = df.select_dtypes(include=['object', 'category'])
    if not categorical_df.empty:
        categorical_summary = categorical_df.describe().T
        print("\n NON-NUMERIC (CATEGORICAL) ATTRIBUTES SUMMARY:")
        print(categorical_summary.to_string())
    else:
        print("\n No non-numeric attributes found.")
        categorical_summary = pd.DataFrame()

    return numeric_summary, categorical_summary


#  Run for each dataset
collision_num_stats, collision_cat_stats = descriptive_statistics(collisions_df, "Collision")
vehicle_num_stats, vehicle_cat_stats     = descriptive_statistics(vehicles_df, "Vehicle")
casualty_num_stats, casualty_cat_stats   = descriptive_statistics(casualties_df, "Casualty")

"""2-E - e. Two interesting and meaningful visualisations (per dataset). Examples include, but are not limited to:

Collision dataset:
i.	Trend of accidents per year/month
ii.	Heatmap of accident density by latitude/longitude

Vehicles dataset:
i.	Stacked bar chart of vehicle types vs. accident severity.
ii.	Correlation matrix of numerical fields

Casualties dataset:
i.	Age distribution of casualties by severity.
ii.	Multiple bar chart of casualty severity vs. casualty class

Two charts: (1) Line plot of monthly accident trends over time, (2) Geographic heatmap showing accident density by location using coordinates.
"""

# ============================================================
# COLLISION DATASET VISUALIZATIONS
# ============================================================

#  Trend of accidents per year/month
collisions_df['date'] = pd.to_datetime(collisions_df['date'], errors='coerce')

# Group by Year and Month
trend_df = collisions_df.groupby(collisions_df['date'].dt.to_period('M')).size().reset_index(name='accident_count')
trend_df['date'] = trend_df['date'].dt.to_timestamp()

plt.figure(figsize=(12, 5))
sns.lineplot(data=trend_df, x='date', y='accident_count', marker='o')
plt.title("Trend of Accidents per Month")
plt.xlabel("Date (Year-Month)")
plt.ylabel("Number of Accidents")
plt.grid(True)
plt.tight_layout()
plt.show()

# Heatmap of accident density by latitude/longitude
plt.figure(figsize=(8, 6))
sns.kdeplot(
    data=collisions_df,
    x='longitude',
    y='latitude',
    fill=True,
    cmap='Reds',
    thresh=0.05
)
plt.title("Heatmap of Accident Density by Location")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.tight_layout()

"""(1) Stacked bar chart of vehicle types across severity levels after merging datasets, (2) Correlation heatmap of numeric vehicle attributes."""

# ============================================================
# VEHICLE DATASET VISUALIZATIONS
# ============================================================

# Merge with collision data to get accident severity
vehicle_severity_df = pd.merge(
    vehicles_df,
    collisions_df[['collision_index', 'collision_severity']],
    on='collision_index',
    how='left'
)

#  Stacked bar chart of vehicle types vs. accident severity
vehicle_severity_count = vehicle_severity_df.groupby(['vehicle_type', 'collision_severity']).size().unstack(fill_value=0)

vehicle_severity_count.plot(
    kind='bar',
    stacked=True,
    figsize=(10, 6)
)
plt.title("Vehicle Types vs. Accident Severity (Stacked Bar)")
plt.xlabel("Vehicle Type")
plt.ylabel("Number of Accidents")
plt.legend(title='Collision Severity')
plt.tight_layout()
plt.show()

#  Correlation matrix of numerical fields
numeric_vehicle_df = vehicles_df.select_dtypes(include=[np.number])
corr_matrix = numeric_vehicle_df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
plt.title("Correlation Matrix - Vehicle Dataset")
plt.tight_layout()
plt.show()

"""(1) Stacked histogram of casualty ages by severity level, (2) Grouped bar chart comparing severity across casualty classes (driver/passenger/pedestrian)."""

# ============================================================
# CASUALTY DATASET VISUALIZATIONS
# ============================================================

# --- i. Age distribution of casualties by severity ---
plt.figure(figsize=(10, 6))
sns.histplot(
    data=casualties_df,
    x='age_of_casualty',
    hue='casualty_severity',
    multiple='stack',
    bins=30
)
plt.title("Age Distribution of Casualties by Severity")
plt.xlabel("Age of Casualty")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# --- ii. Multiple bar chart of casualty severity vs. casualty class ---
plt.figure(figsize=(10, 6))
sns.countplot(
    data=casualties_df,
    x='casualty_class',
    hue='casualty_severity'
)
plt.title("Casualty Severity vs. Casualty Class")
plt.xlabel("Casualty Class")
plt.ylabel("Count")
plt.legend(title='Casualty Severity')
plt.tight_layout()
plt.show()

"""3.	**Data Cleaning & Feature Engineering: For each dataset (Collisions, Vehicles, Casualties).**

a.	Implement any ONE data cleaning procedure. For example,

i.	Removing columns of redundant attributes

ii.	Dealing with outliers

iii.	Filtering or sampling data

(6marks)


b.	Implement any TWO feature engineering procedures. For example,

i.	Handling missing values by either removing rows with missing data or imputing values.

ii.	Encoding at least one categorical variable.

iii.	Aggregating data

iv.	Creating at least one new meaningful attribute from already existing attributes

(12marks)

Removes redundant OSGR columns, fills missing speed limits with median, and creates weighted 'accident_risk_index' combining vehicles, casualties, and severity.
"""

# ============================================================
# COLLISION DATASET - DATA CLEANING
# ============================================================

# Remove redundant columns related to location
collisions_cleaned = collisions_df.drop(columns=['location_easting_osgr', 'location_northing_osgr'], errors='ignore')

print("Removed redundant columns. New shape:", collisions_cleaned.shape)

# ============================================================
# COLLISION DATASET - FEATURE ENGINEERING
# ============================================================

# Replace missing speed limits with median
collisions_cleaned['speed_limit'] = collisions_cleaned['speed_limit'].fillna(collisions_cleaned['speed_limit'].median())


# Create a new derived attribute: Accident Risk Index
collisions_cleaned['accident_risk_index'] = (
    collisions_cleaned['number_of_vehicles'] * 0.6 +
    collisions_cleaned['number_of_casualties'] * 1.2 +
    collisions_cleaned['collision_severity'] * 2
)

print(collisions_cleaned[['collision_index', 'accident_risk_index']].head())

"""Filters ages to 17-90 range, imputes missing engine capacity with mean, and one-hot encodes propulsion type into binary columns."""

# ============================================================
# VEHICLE DATASET - DATA CLEANING
# ============================================================

vehicles_cleaned = vehicles_df[(vehicles_df['age_of_driver'] >= 17) & (vehicles_df['age_of_driver'] <= 90)]
print("Removed unrealistic driver ages. New shape:", vehicles_cleaned.shape)


# ============================================================
# VEHICLE DATASET - FEATURE ENGINEERING
# ============================================================

#Impute missing engine capacities with mean
vehicles_cleaned['engine_capacity_cc'] = vehicles_cleaned['engine_capacity_cc'].fillna(vehicles_cleaned['engine_capacity_cc'].mean())


# Example encoding categorical 'propulsion_code'
propulsion_encoded = pd.get_dummies(vehicles_cleaned['propulsion_code'], prefix='propulsion')
vehicles_encoded = pd.concat([vehicles_cleaned, propulsion_encoded], axis=1)

print("Encoded propulsion_code. New columns added:", propulsion_encoded.columns.tolist()[:5])

"""Samples 10% of data, drops missing ages, and creates categorical 'age_group' feature by binning continuous ages into 6 demographic categories."""

# ============================================================
# CASUALTY DATASET - DATA CLEANING
# ============================================================

casualties_cleaned = casualties_df.sample(frac=0.1, random_state=42)
print("Sampled 10% of casualty records. New shape:", casualties_cleaned.shape)


# ============================================================
# CASUALTY DATASET - FEATURE ENGINEERING
# ============================================================

#Remove rows with missing age
casualties_cleaned = casualties_cleaned.dropna(subset=['age_of_casualty'])


# Define age groups
bins = [0, 12, 18, 30, 50, 70, 100]
labels = ['Child', 'Teen', 'Young Adult', 'Adult', 'Senior', 'Elderly']
casualties_cleaned['age_group'] = pd.cut(casualties_cleaned['age_of_casualty'], bins=bins, labels=labels, right=False)

print(casualties_cleaned[['age_of_casualty', 'age_group']].head())

"""# SUMMARY TABLE OF PROCESSING Q3

| Dataset        | Data Cleaning                          | Feature Engineering #1                      | Feature Engineering #2        |
| -------------- | -------------------------------------- | ------------------------------------------- | ----------------------------- |
| **Collisions** | Removed redundant columns              | Imputed missing `speed_limit`               | Created `accident_risk_index` |
| **Vehicles**   | Removed outliers (invalid driver ages) | Imputed missing `engine_capacity_cc`        | Encoded `propulsion_code`     |
| **Casualties** | Sampled 10% of data                    | Dropped rows with missing `age_of_casualty` | Created `age_group`           |

4.	**Data Transformation:**


**b. Write a Python script to:**

i. 	Create dimensions: Collision, Casualties, Vehicle and Time
(12marks)

ii. 	Create a fact table
(5marks)

Creates 4 normalized dimensions (Collision, Vehicle, Casualty, Time) with unique keys, removing duplicates and organizing data for star schema warehouse structure.
"""

# ============================================================
# CREATE DIMENSIONS
# ============================================================

# DIM_COLLISION
dim_collision = collisions_cleaned[[
    'collision_index',
    'collision_severity',
    'weather_conditions',
    'road_type',
    'speed_limit',
    'urban_or_rural_area',
    'longitude',
    'latitude'
]].drop_duplicates().reset_index(drop=True)
dim_collision['collision_key'] = dim_collision.index + 1

# DIM_VEHICLE
dim_vehicle = vehicles_encoded[[
    'collision_index',
    'vehicle_reference',
    'vehicle_type',
    'engine_capacity_cc',
    'propulsion_code',
    'age_of_driver',
    'sex_of_driver'
]].drop_duplicates().reset_index(drop=True)
dim_vehicle['vehicle_key'] = dim_vehicle.index + 1

# DIM_CASUALTY
dim_casualty = casualties_cleaned[[
    'collision_index',
    'vehicle_reference',
    'casualty_reference',
    'casualty_class',
    'sex_of_casualty',
    'age_of_casualty',
    'age_group',
    'casualty_severity'
]].drop_duplicates().reset_index(drop=True)
dim_casualty['casualty_key'] = dim_casualty.index + 1

# DIM_TIME
collisions_cleaned['date'] = pd.to_datetime(collisions_cleaned['date'], errors='coerce')
dim_time = pd.DataFrame({
    'time_key': range(1, len(collisions_cleaned) + 1),
    'collision_index': collisions_cleaned['collision_index'],
    'date': collisions_cleaned['date'],
    'year': collisions_cleaned['date'].dt.year,
    'month': collisions_cleaned['date'].dt.month,
    'day_of_week': collisions_cleaned['date'].dt.day_name(),
    'hour': collisions_cleaned['time'].str.split(':').str[0].astype(float, errors='ignore')
})

"""Merges dimension keys (collision_key, time_key) with measurable facts (total_vehicles, total_casualties, accident_count). Central table for star schema analytics."""

# ============================================================
# CREATE FACT TABLE
# ============================================================

# Merge necessary dimension keys
fact_accident = collisions_cleaned[['collision_index', 'number_of_vehicles', 'number_of_casualties']].copy()

# Add keys from dimension tables
fact_accident = fact_accident.merge(dim_collision[['collision_index', 'collision_key']], on='collision_index', how='left')
fact_accident = fact_accident.merge(dim_time[['collision_index', 'time_key']], on='collision_index', how='left')

# Aggregate measures
fact_accident['accident_count'] = 1  # Each record = 1 collision
fact_accident.rename(columns={
    'number_of_vehicles': 'total_vehicles',
    'number_of_casualties': 'total_casualties'
}, inplace=True)

# Finalize
fact_accident = fact_accident[['collision_key', 'time_key', 'total_vehicles', 'total_casualties', 'accident_count']]

print("Fact table created successfully.")
fact_accident.head()

"""5. **Data Loading:** Choose a relational database system (e.g., SQLite, MySQL, PostgreSQL). Create the necessary database schema and tables to store the transformed data. Implement code to efficiently load the transformed data into the database. 						(4marks)

Imports libraries, creates/connects to "collision_analysis.db", establishes cursor for SQL execution, and confirms database location.
"""

import sqlite3
from pathlib import Path

# ============================================================
# 1. Create Database
# ============================================================

# Define database file path
db_path = Path("collision_analysis.db")

# Connect to SQLite (creates the database if not exists)
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

print("Connected to SQLite database:", db_path.resolve())

"""Executes SQL to create 4 dimension tables (collision, vehicle, casualty, time) and 1 fact table with foreign key constraints. Commits and confirms success."""

#-----------------------------------------------
#Create Tables
#-----------------------------------------------

# Create DIM_COLLISION
cursor.execute("""
CREATE TABLE IF NOT EXISTS dim_collision (
    collision_key INTEGER PRIMARY KEY,
    collision_index TEXT,
    collision_severity INTEGER,
    weather_conditions INTEGER,
    road_type INTEGER,
    speed_limit REAL,
    urban_or_rural_area INTEGER,
    longitude REAL,
    latitude REAL
)
""")

# Create DIM_VEHICLE
cursor.execute("""
CREATE TABLE IF NOT EXISTS dim_vehicle (
    vehicle_key INTEGER PRIMARY KEY,
    collision_index TEXT,
    vehicle_reference INTEGER,
    vehicle_type INTEGER,
    engine_capacity_cc REAL,
    propulsion_code INTEGER,
    age_of_driver INTEGER,
    sex_of_driver INTEGER
)
""")

# Create DIM_CASUALTY
cursor.execute("""
CREATE TABLE IF NOT EXISTS dim_casualty (
    casualty_key INTEGER PRIMARY KEY,
    collision_index TEXT,
    vehicle_reference INTEGER,
    casualty_reference INTEGER,
    casualty_class INTEGER,
    sex_of_casualty INTEGER,
    age_of_casualty INTEGER,
    age_group TEXT,
    casualty_severity INTEGER
)
""")

# Create DIM_TIME
cursor.execute("""
CREATE TABLE IF NOT EXISTS dim_time (
    time_key INTEGER PRIMARY KEY,
    collision_index TEXT,
    date TEXT,
    year INTEGER,
    month INTEGER,
    day_of_week TEXT,
    hour REAL
)
""")

# Create FACT_ACCIDENT
cursor.execute("""
CREATE TABLE IF NOT EXISTS fact_accident (
    collision_key INTEGER,
    time_key INTEGER,
    total_vehicles INTEGER,
    total_casualties INTEGER,
    accident_count INTEGER,
    FOREIGN KEY(collision_key) REFERENCES dim_collision(collision_key),
    FOREIGN KEY(time_key) REFERENCES dim_time(time_key)
)
""")

conn.commit()
print("All tables created successfully.")

"""Writes all dimension and fact DataFrames to SQLite using `to_sql()` with replace mode. Confirms successful data warehouse population."""

#---------------------------------------------------
#LOAD DATA INTO TABLES
#-----------------------------------------------

# Load DIM tables
dim_collision.to_sql('dim_collision', conn, if_exists='replace', index=False)
dim_vehicle.to_sql('dim_vehicle', conn, if_exists='replace', index=False)
dim_casualty.to_sql('dim_casualty', conn, if_exists='replace', index=False)
dim_time.to_sql('dim_time', conn, if_exists='replace', index=False)

# Load FACT table
fact_accident.to_sql('fact_accident', conn, if_exists='replace', index=False)

print("All transformed data loaded into SQLite successfully.")

"""6.	Data pipeline testing: Implement tests for each stage of the data pipeline to ensure correctness, reliability, and reproducibility.

a.	Data loader test: Implement any ONE of the testing procedures. Examples

  include, but are not limited to:

  i.	Test that the raw data files can be loaded successfully

  ii.	Verify the expected number of columns.

b.	Transformation testing: Implement any ONE of the testing procedures.  
  Examples include, but are not limited to:

  i.	Verify cleaning rules are applied correctly, e.g., no negative ages in the driver column.

  ii.	Encoded categorical columns have expected categories.

c.	Serving/loading tests: Implement any ONE of the testing procedures.
  Examples include, but are not limited to:

  i.	Test that transformed data can be inserted into the relational database.

  ii.	Verify that fact and dimension tables contain data after loading

(6marks)

Validates all driver ages are 17-90. Uses assertion to fail if invalid ages exist, otherwise confirms transformation success.
"""

# ============================================================
# TRANSFORMATION TEST
# ============================================================

# All ages should be between 17 and 90
invalid_ages = vehicles_cleaned[(vehicles_cleaned['age_of_driver'] < 17) |
                                (vehicles_cleaned['age_of_driver'] > 90)]

assert invalid_ages.empty, f"Transformation test failed: found invalid driver ages:\n{invalid_ages}"
print("Transformation test passed: all driver ages are within expected range.")

"""Validates one-hot encoding created 'propulsion_' prefixed columns. Assertion fails if encoding didn't execute properly."""

# Check propulsion_code encoding exists
encoded_cols = [col for col in vehicles_encoded.columns if col.startswith('propulsion_')]
assert len(encoded_cols) > 0, "Transformation test failed: propulsion_code was not encoded."

"""Queries row counts for all 5 tables. Assertion fails if any table is empty, confirms success if all contain data."""

# ============================================================
# SERVING / LOADING TEST
# ============================================================

# Test that dimension tables contain data
tables_to_check = ['dim_collision', 'dim_vehicle', 'dim_casualty', 'dim_time', 'fact_accident']
for table in tables_to_check:
    cursor.execute(f"SELECT COUNT(*) FROM {table}")
    count = cursor.fetchone()[0]
    assert count > 0, f"Serving/loading test failed: {table} is empty."

print("Serving/loading test passed: all tables contain data.")

"""Compares row counts between fact_accident and dim_collision. Assertion fails if counts don't match, indicating data integrity issues."""

#Test fact table consistency (total collisions match dimension collisions):

cursor.execute("SELECT COUNT(*) FROM fact_accident")
fact_count = cursor.fetchone()[0]
cursor.execute("SELECT COUNT(*) FROM dim_collision")
dim_count = cursor.fetchone()[0]
assert fact_count == dim_count, "Fact table rows do not match collision dimension rows."

"""# Summary of Tests Implemented for Q6

| Stage               | Test Implemented                                     | Method                                                           |
| ------------------- | ---------------------------------------------------- | ---------------------------------------------------------------- |
| **Data Loader**     | Verify raw data loaded & expected columns present    | `assert collisions_df` + `set` check                             |
| **Transformation**  | No negative driver ages; categorical encoding exists | `assert vehicles_cleaned['age_of_driver'] >= 17` & one-hot check |
| **Serving/Loading** | Tables contain rows after load                       | `SELECT COUNT(*)` from each table + comparison with fact table   |

"""